{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENIGH Data\n",
    "\n",
    "This notebook extracts and cleans the data of the Income and Expenditure Survey. Such files can be downloaded from the INEGI's website with the following link:\n",
    "https://www.inegi.org.mx/programas/enigh/nc/2018/default.html#Microdatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractENIGH(dataRoot, year, urban=True):\n",
    "    ''' \n",
    "    Extracts data from the ENIGH tables, these must be previously downloaded and stored in a folder with\n",
    "    the folder name equal to the corresponding year of the ENIGH data (Example: \"Data\\2018\\hogar.csv\"). The \n",
    "    datapoints (households) are merged with the metropolitan area where they are located (by default: inner merge,\n",
    "    urban=False, left merge). All the data taken from the tables is returned as a single dataFrame.\n",
    "    \n",
    "    -dataRoot: root folder where the data is located\n",
    "    -year: year and name folder where the data is extracted (dataRoot\\year\\example.csv)\n",
    "    -urban: if True the dataFrame only includes urban households, if false, returns all households.\n",
    "    '''\n",
    "        \n",
    "    # file path\n",
    "    tabConHogar = r'/Ingresos y Gastos de los Hogares/' + year + '/concentradohogar.csv'\n",
    "    tabPoblacion = r'\\Ingresos y Gastos de los Hogares/' + year + '/poblacion.csv'\n",
    "    tabViviendas = r'\\Ingresos y Gastos de los Hogares/' + year + '/viviendas.csv'\n",
    "    tabGastosH = r'\\Ingresos y Gastos de los Hogares/' + year + '/gastoshogar.csv'\n",
    "    tabGastosP = r'\\Ingresos y Gastos de los Hogares/' + year + '/gastospersona.csv'\n",
    "    \n",
    "    if year=='2018':\n",
    "        concentrador_cols = list(range(17)) + [19, 22, 56,82,92] # columns of interest\n",
    "    elif year=='2016':\n",
    "        concentrador_cols = [0,1,2] + list(range(4,18)) + [20, 23, 57,83,93] # columns of interest\n",
    "    else:\n",
    "        raise ValueError('There is no data for the given year')\n",
    "        \n",
    "        \n",
    "    # Read files\n",
    "    concentrador = pd.read_csv(dataRoot + tabConHogar, usecols=concentrador_cols, na_values=' ') # read tabla concentrador hogar\n",
    "    if urban:\n",
    "        concentrador = concentrador.loc[concentrador['tam_loc']!=4]\n",
    "    if year == '2016':\n",
    "        concentrador.ubica_geo = concentrador.ubica_geo.astype(str).str[:-4].astype('int64')\n",
    "        concentrador.insert(3, \"estado\", concentrador.ubica_geo.astype(str).str[:-3].astype(int))\n",
    "    else:\n",
    "        concentrador.insert(3, \"estado\", concentrador.ubica_geo.astype(str).str[:-3].astype(int)) \n",
    "\n",
    "    poblacion_cols = [0,1,2,5,40] # columns of interest\n",
    "    poblacion = pd.read_csv(dataRoot + tabPoblacion, usecols=poblacion_cols, na_values=' ') # read tabla poblacion\n",
    "\n",
    "    vivienda_cols = [0,1,5,10] + list(range(21,25)) + [27,46,47,50,51,52]  # columns of interest\n",
    "    vivienda = pd.read_csv(dataRoot + tabViviendas, usecols=vivienda_cols, na_values=['&',' '], dtype={'combustible': float}) # read tabla vivienda\n",
    "\n",
    "    gastosH_cols = [0,1,2,23]\n",
    "    gastosH = pd.read_csv(dataRoot + tabGastosH, usecols=gastosH_cols, na_values=' ') # read tabla gastoshogar\n",
    "    gastosH = gastosH.groupby(['folioviv', 'foliohog', 'clave'],as_index=False).sum(min_count=1)\n",
    "\n",
    "    gastosP_cols = [0,1,3,17]\n",
    "    gastosP = pd.read_csv(dataRoot + tabGastosP, usecols=gastosP_cols, na_values=' ') # read tabla gastoshogar\n",
    "    gastosP = gastosP.groupby(['folioviv', 'foliohog', 'clave'],as_index=False).sum(min_count=1)\n",
    "    \n",
    "    \n",
    "    # Add metropolitan area code and municpality name\n",
    "    ZM_2015 = pd.read_csv(dataRoot + \"\\Zonas metropolitanas\\ZM_2015.csv\", encoding='latin-1', usecols=list(range(6)))\n",
    "    if urban:\n",
    "        concentrador = concentrador.merge(ZM_2015[['CVE_ZM','NOM_ZM','CVE_ENT','NOM_ENT','CVE_MUN','NOM_MUN']], \n",
    "                                      left_on='ubica_geo', right_on='CVE_MUN', how='inner')\n",
    "    else:\n",
    "        concentrador = concentrador.merge(ZM_2015[['CVE_ZM','NOM_ZM','CVE_ENT','NOM_ENT','CVE_MUN','NOM_MUN']], \n",
    "                                      left_on='ubica_geo', right_on='CVE_MUN', how='left')\n",
    "    # fix column order    \n",
    "    concentrador = concentrador[list(concentrador)[:4]+list(concentrador)[-6:]+list(concentrador)[4:-6]]\n",
    "    \n",
    "    \n",
    "    # Transform values to school years\n",
    "    dic_edu = {0:0,1:0,2:6,3:9,4:12,5:15,6:15,7:16,8:18,9:21} # for mapping the number of school years equivalent to each value \n",
    "    poblacion['nivelaprob'] = poblacion['nivelaprob'].map(dic_edu)\n",
    "\n",
    "    # Age\n",
    "    pobEdad = poblacion.groupby(['folioviv','foliohog'], sort=False, as_index=False)['edad'].mean() # Get average age\n",
    "\n",
    "    # Education\n",
    "    pobEdu = poblacion.loc[poblacion['edad']>=15].groupby(['folioviv','foliohog'], \n",
    "                                sort=False, as_index=False)['nivelaprob'].mean() # Get mean education of population > 15 y\n",
    "    pobEdu_men15 = poblacion.groupby(['folioviv','foliohog'], \n",
    "                                sort=False, as_index=False)['nivelaprob'].mean() # Get mean poblacion_ of whole population to fill houses with no >15 population\n",
    "\n",
    "    # merge\n",
    "    pob = pobEdad.merge(pobEdu, on=['folioviv','foliohog'], how='left')\n",
    "    pob['nivelaprob'] = pob['nivelaprob'].fillna(pobEdu_men15['nivelaprob']) # give a value for households with no population older than 15 years old\n",
    "    \n",
    "    # Merge with concentrador Hogar\n",
    "    con = concentrador.merge(pob, on=['folioviv','foliohog'], how='left')\n",
    "    \n",
    "    \n",
    "    # Housing\n",
    "    con = con.merge(vivienda, on='folioviv', how='left') # Merge dataframes\n",
    "\n",
    "    \n",
    "    # Expenses' keys\n",
    "    claves = ['R001','R003','G009','G010','G011','G012','G013','G014']\n",
    "    dic_cves = {'R001':'ele', 'R003':'gas', 'G009':'lpg', 'G010':'oil', 'G011':'diesel', 'G012':'coal', \n",
    "                'G013':'wood', 'G014':'heat', 'F007':'Magna', 'F008':'Premium', 'F009':'Die-Gas'}\n",
    "\n",
    "    # Sort every type of expenses into columns\n",
    "    def mergeGastos(gastoDF):\n",
    "        gastoList = []\n",
    "        for clave in dic_cves:\n",
    "            gasto_x = gastoDF.loc[gastoDF['clave']==clave].copy()\n",
    "            gasto_x.rename(columns={'gasto_tri':'gasto_tri_' + dic_cves[clave]}, inplace=True)\n",
    "            gasto_x.drop(['clave'], axis=1, inplace=True)\n",
    "            gastoList.append(gasto_x)\n",
    "\n",
    "        # merge all the expenses\n",
    "        return reduce(lambda  left,right: pd.merge(left,right, on=['folioviv','foliohog'], how='outer'), gastoList)\n",
    "\n",
    "    gastH = mergeGastos(gastosH)\n",
    "    gastP = mergeGastos(gastosP)\n",
    "\n",
    "    # Sum expenses from hogar and persona\n",
    "    gasto = gastH.set_index(['folioviv', 'foliohog']).add(gastP.set_index(['folioviv', 'foliohog']), fill_value=0)\n",
    "    gasto.reset_index(inplace=True)\n",
    "\n",
    "    # Merge with concentrador Hogar\n",
    "    con = con.merge(gasto, on=['folioviv','foliohog'], how='left')\n",
    "    \n",
    "    return con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction\n",
    "Give the root folder of the files and the version (year) of the ENIGH survey to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRoot = r'D:\\Tesis\\Datos' #path of data folder\n",
    "year = '2018'\n",
    "con_2018 = extractENIGH(dataRoot, year,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2016'\n",
    "con_2016 = extractENIGH(dataRoot, year, urban=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_df = con_2018 # name of dataframe\n",
    "\n",
    "# Columns for completeness check\n",
    "subset = list(con_df)[list(con_df).index('folioviv'):list(con_df).index('factor')]+['publico'] + \\\n",
    "            list(con_df)[list(con_df).index('disp_elect'):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_completeness = con_df[subset].notnull().sum() * 100 / len(con_df[subset])\n",
    "print(percent_completeness,'\\n','-'*30)\n",
    "\n",
    "keyDict = {1:'public', 2:'priv plant', 3:'solar', 4:'other', 5:'no elec'}\n",
    "s = (con_df['disp_elect'].value_counts(normalize=True, dropna=False) * 100).to_frame()\n",
    "s['key'] = s.index.to_series().map(keyDict)\n",
    "print(s,'\\n','-'*30)\n",
    "\n",
    "keyDict = {1:'wood', 2:'coal', 3:'gas tank', 4:'gas pipe', 5:'elec', 6:'other'}\n",
    "s = (con_df['combustible'].value_counts(normalize=True, dropna=False) * 100).to_frame()\n",
    "s['key'] = s.index.to_series().map(keyDict)\n",
    "print(s,'\\n','-'*30)\n",
    "\n",
    "keyDict = {1:'yes', 2:'no'}\n",
    "s = (con_df['calent_sol'].value_counts(normalize=True, dropna=False) * 100).to_frame()\n",
    "s['key'] = s.index.to_series().map(keyDict)\n",
    "print(s,'\\n','-'*30)\n",
    "\n",
    "s = (con_df['calent_gas'].value_counts(normalize=True, dropna=False) * 100).to_frame()\n",
    "s['key'] = s.index.to_series().map(keyDict)\n",
    "print(s,'\\n','-'*30)\n",
    "\n",
    "s = (con_df['tanque_gas'].value_counts(normalize=True, dropna=False) * 100).to_frame()\n",
    "s['key'] = s.index.to_series().map(keyDict)\n",
    "print(s,'\\n','-'*30)\n",
    "\n",
    "s = (con_df['aire_acond'].value_counts(normalize=True, dropna=False) * 100).to_frame()\n",
    "s['key'] = s.index.to_series().map(keyDict)\n",
    "print(s,'\\n','-'*30)\n",
    "\n",
    "s = (con_df['calefacc'].value_counts(normalize=True, dropna=False) * 100).to_frame()\n",
    "s['key'] = s.index.to_series().map(keyDict)\n",
    "print(s,'\\n','-'*30)\n",
    "\n",
    "keyDict = {1:'rented', 2:'lend', 3:'own and paying', 4:'own', 5:'litigated', 6:'other'}\n",
    "s = (con_df['tenencia'].value_counts(normalize=True, dropna=False) * 100).to_frame()\n",
    "s['key'] = s.index.to_series().map(keyDict)\n",
    "print(s,'\\n','-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data\n",
    "Choose the path to save the dataframe in pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkls_path = pkls = r'D:\\Tesis\\ResEleCon-MX\\pickles'\n",
    "con_df.to_pickle(pkls + '\\concentrador.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Metropolis/State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRoot = r'D:\\Tesis\\Datos' #path of data folder\n",
    "year = '2018'\n",
    "con_state = extractENIGH(dataRoot, year, urban=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZM_2015 = pd.read_csv(dataRoot + \"\\Zonas metropolitanas\\ZM_2015.csv\", encoding='latin-1', usecols=list(range(6)))\n",
    "zm_mapper = ZM_2015[['CVE_ZM','NOM_ZM']].drop_duplicates().set_index('CVE_ZM').to_dict()['NOM_ZM']\n",
    "st_mapper = ZM_2015[['CVE_ENT','NOM_ENT']].drop_duplicates().set_index('CVE_ENT').to_dict()['NOM_ENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = []\n",
    "for state in con_state['estado'].unique():\n",
    "    for metro in con_state.loc[(con_state['estado']==state) & (con_state['CVE_ZM'].notnull()) , 'CVE_ZM'].unique():\n",
    "        state_list.append([state, st_mapper[state], metro, zm_mapper[metro], \n",
    "                          len(con_state[(con_state['CVE_ZM']==metro) & (con_state['tam_loc']!=4)\n",
    "                                       & (con_state['estado']==state)])/len(con_state[con_state['estado']==state])])\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stateMetro = pd.DataFrame(state_list, columns =['State cod','StateName','Metropolitan code', 'Metropolitan name','state_ratio']) \n",
    "stateMetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stateMetro.to_clipboard(index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
